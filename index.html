<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Haochen Shi</title>
  <meta name="author" content="Haochen Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVK6EML3WQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-KVK6EML3WQ');
  </script>
</head>

<body>
  <section class="introduction">
    <div class="bio">
      <h2 class="name">Haochen Shi</h2>
      <p>
        I'm a first-year CS Ph.D. student at Stanford advised by <a
          href="https://tml.stanford.edu/people/karen-liu">Karen Liu</a>. I worked with <a
          href="https://jiajunwu.com/">Jiajun Wu</a>, <a href="http://hxu.rocks/">Huazhe Xu</a>,
        <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
        on robotics and computer vision, specifically deformable object manipulation, during my Master's at Stanford and
        <a href="https://gleicher.sites.cs.wisc.edu/">Michael Gleicher</a> and
        <a href="https://dannyrakita.net/">Danny Rakita</a> on robotics and motion planning during my
        undergraduate at UW-Madison.
      </p>
      <p>
        I'm interested in challenging robotic manipulation tasks.
        In my leisure time, I love playing tennis, Go, chess, and mahjong.
      </p>
      <p style="text-align:center">
        <a href="mailto:hshi74@stanford.edu">Email</a> &nbsp/&nbsp
        <a href="https://scholar.google.com/citations?hl=en&user=gr51Y7UAAAAJ">Google Scholar</a>
        &nbsp/&nbsp
        <a href="https://twitter.com/HaochenShi74">Twitter</a> &nbsp/&nbsp
        <a href="https://github.com/hshi74">Github</a>
      </p>
    </div>
    <img class="profile-photo" src="images/me.jpeg" alt="Haochen Shi">
  </section>

  <section class="research">
    <h2>Research</h2>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="images/robocraft3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCraft: Learning to see, simulate, and shape elasto-plastic objects in 3D with graph
          networks</b>
        <p class="paper-authors">
          <b>Haochen Shi*</b>,
          <a href="http://hxu.rocks/">Huazhe Xu*</a>,
          <a href="https://sites.google.com/view/zhiao-huang/">Zhiao Huang</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><em>The International Journal of Robotics Research (IJRR)</em></p>
        <div class="paper-links">
          <a href="https://doi.org/10.1177/02783649231219020">[Paper]</a>
          <a href="https://github.com/hshi74/robocraft3d">[Code]</a>
        </div>
        <p class="paper-description">Modeling and manipulating elasto-plastic objects are essential capabilities for
          robots to perform complex industrial and household interaction tasks (e.g., stuffing dumplings, rolling sushi,
          and making pottery). However, due to the high degrees of freedom of elasto-plastic objects, significant
          challenges exist in virtually every aspect of the robotic manipulation pipeline, e.g., representing the
          states, modeling the dynamics, and synthesizing the control signals. We propose to tackle these challenges by
          employing a particle-based representation for elasto-plastic objects in a model-based planning framework.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="images/robocook.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</b>
        <p class="paper-authors">
          <b>Haochen Shi*</b>,
          <a href="http://hxu.rocks/">Huazhe Xu*</a>,
          <a href="https://samuelpclarke.com/">Samuel Clarke</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><em>CoRL</em>, 2023</p>
        <b class="paper-award">Best System Paper</b>
        <div class="paper-links">
          <a href="https://hshi74.github.io/robocook">[Project Page]</a>
          <a href="https://arxiv.org/abs/2306.14447">[Paper]</a>
          <a href="https://github.com/hshi74/robocook">[Code]</a>
        </div>
        <p class="paper-description">Humans excel in complex long-horizon soft body manipulation tasks via flexible tool
          use: bread baking requires a knife to slice the dough and a rolling pin to flatten it. Often regarded as a
          hallmark of human cognition, tool use in autonomous robots remains limited due to challenges in
          understanding tool-object interactions. Here we develop an intelligent robotic system, RoboCook,
          which perceives, models, and manipulates elasto-plastic objects with various tools.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="images/rtx.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</b>
        <p class="paper-authors">
          <a href="https://robotics-transformer-x.github.io/">Open X-Embodiment Collaboration</a>
        </p>
        <p class="paper-details"><em>preprint</em>, 2023</p>
        <div class="paper-links">
          <a href="https://robotics-transformer-x.github.io">[Project Page]</a>
          <a href="https://robotics-transformer-x.github.io/paper.pdf">[Paper]</a>
        </div>
        <p class="paper-description">Large, high-capacity models trained on diverse datasets have shown remarkable
          successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision,
          this has led to a consolidation of pretrained models, with general pretrained backbones
          serving as a starting point for many applications.
          Can such a consolidation happen in robotics?
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="images/robocraft.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph
          Networks</b>
        <p class="paper-authors">
          <b>Haochen Shi*</b>,
          <a href="http://hxu.rocks/">Huazhe Xu*</a>,
          <a href="https://sites.google.com/view/zhiao-huang/">Zhiao Huang</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><em>RSS</em>, 2022</p>
        <p class="paper-details">Abridged in ICRA 2022 workshop on Representing and Manipulating Deformable Objects</p>
        <div class="paper-links">
          <a href="http://hxu.rocks/robocraft">[Project Page]</a>
          <a href="https://arxiv.org/abs/2205.02909">[Paper]</a>
          <a href="https://github.com/hshi74/RoboCraft">[Code]</a>
        </div>
        <p class="paper-details">
          Covered by
          <a href="https://news.mit.edu/2022/robots-play-play-dough-0623">[MIT News]</a>
          <a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh">[Stanford HAI]</a>
          <a
            href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/">[New
            Scientist]</a>
        </p>
        <p class="paper-description">Modeling and manipulating elasto-plastic objects are essential capabilities
          for robots to perform complex industrial and household interaction tasks. However, significant
          challenges exist in virtually every aspect of the robotic manipulation pipeline,
          e.g., representing the states, modeling the dynamics, and synthesizing
          the control signals. We propose to tackle these challenges by employing a
          particle-based representation for elasto-plastic objects in a model-based planning framework.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="images/cik.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">CollisionIK: A per-instant pose optimization method for generating robot motions with
          environment collision avoidance</b>
        <p class="paper-authors">
          <a href="https://pages.cs.wisc.edu/~rakita/">Daniel Rakita</a>,
          <b>Haochen Shi</b>,
          <a href="http://bilgemutlu.com/">Bilge Mutlu</a>,
          <a href="https://gleicher.sites.cs.wisc.edu/">Michael Gleicher</a>
        </p>
        <p class="paper-details"><em>ICRA</em>, 2021</p>
        <div class="paper-links">
          <a href="https://arxiv.org/abs/2102.13187">[Paper]</a>
          <a href="https://github.com/uwgraphics/relaxed_ik_core">[Code]</a>
          <a href="https://youtu.be/rdMl1gOPNoM">[Talk]</a>
        </div>
        <p class="paper-description">In this work, we present a per-instant pose optimization method that can generate
          configurations that achieve specified pose or motion objectives as best as possible over a sequence of
          solutions, while also simultaneously avoiding collisions with static or dynamic obstacles in the environment.
        </p>
    </article>
  </section>

  <section class="teaching">
    <h2>Teaching</h2>
    <ul>
      <li>Teaching Assistant, <a href="http://cs231n.stanford.edu/">Stanford CS 231N</a>, Spring 2023</li>
      <li>Teaching Assistant, <a href="http://cs231n.stanford.edu/">Stanford CS 231N</a>, Spring 2022</li>
      <li>Teaching Assistant, <a href="https://web.stanford.edu/class/cs109/">Stanford CS 109</a>, Winter
        2022</li>
      <li>Teaching Assistant, <a href="https://web.stanford.edu/class/cs109/">Stanford CS 109</a>, Fall
        2021</li>
      <li>Peer Mentor, <a href="https://graphics.cs.wisc.edu/Courses/559-sp2020/">UW-Madison CS 559</a>,
        Spring 2020</li>
    </ul>
  </section>

  <section class="talks">
    <h2>Talks</h2>
    <ul>
      <li>[2023/5/28] Invited Talk at Tsinghua University</li>
    </ul>
  </section>

  <section class="service">
    <h2>Service</h2>
    <ul>
      <li>Conference Reviewer: IROS, CoRL</li>
    </ul>
  </section>

  <footer>
    <p>Template from <a href="https://jonbarron.info/">Jon Barron's website</a></p>
  </footer>

</body>

</html>